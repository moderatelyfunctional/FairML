{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIF360/examples.common_utils.py\n",
    "# Metrics function\n",
    "from collections import OrderedDict\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def compute_metrics(dataset_true, dataset_pred, \n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp = True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                 dataset_pred, \n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    metrics[\"Theil index\"] = classified_metric_pred.theil_index()\n",
    "    \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./resume_data.csv')\n",
    "df = df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'GPA', 'Gender', 'Experience (yrs)', 'School', 'Accepted'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Accepted'] = df['Accepted'].replace(0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Experience (yrs)</th>\n",
       "      <th>School</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Sharon Miller</td>\n",
       "      <td>3.680160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Penn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Jennifer Carrington</td>\n",
       "      <td>3.330398</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Mary Crabtree</td>\n",
       "      <td>3.501095</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Rhonda Washburn</td>\n",
       "      <td>3.486283</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Jamie Unger</td>\n",
       "      <td>3.726329</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Charline Williams</td>\n",
       "      <td>3.213889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Doris Kuether</td>\n",
       "      <td>3.702024</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Stacey Brigham</td>\n",
       "      <td>3.414050</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Dorthy Wood</td>\n",
       "      <td>3.635322</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Mary Huffman</td>\n",
       "      <td>3.833324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Jessie Hall</td>\n",
       "      <td>3.642257</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Brenda Tucker</td>\n",
       "      <td>3.560427</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Versie Parker</td>\n",
       "      <td>3.464078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Linda Escobedo</td>\n",
       "      <td>3.110268</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Mary Messing</td>\n",
       "      <td>3.243789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Sheila Smith</td>\n",
       "      <td>3.633974</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Ellen Scrivner</td>\n",
       "      <td>3.322853</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Marisa Kyser</td>\n",
       "      <td>3.353890</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Patricia Cendejas</td>\n",
       "      <td>3.150307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Barnard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Sue Reed</td>\n",
       "      <td>3.206812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Nancy Clayborne</td>\n",
       "      <td>3.028616</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name       GPA  Gender  Experience (yrs)     School  \\\n",
       "57         Sharon Miller  3.680160       0                 1       Penn   \n",
       "59   Jennifer Carrington  3.330398       0                 1  Wellesley   \n",
       "74         Mary Crabtree  3.501095       0                 2    Barnard   \n",
       "114      Rhonda Washburn  3.486283       0                 1    Barnard   \n",
       "150          Jamie Unger  3.726329       0                 4  Wellesley   \n",
       "169    Charline Williams  3.213889       0                 1  Wellesley   \n",
       "175        Doris Kuether  3.702024       0                 4  Wellesley   \n",
       "208       Stacey Brigham  3.414050       0                 2  Wellesley   \n",
       "241          Dorthy Wood  3.635322       0                 2    Barnard   \n",
       "287         Mary Huffman  3.833324       0                 0    Barnard   \n",
       "333          Jessie Hall  3.642257       0                 2    Barnard   \n",
       "361        Brenda Tucker  3.560427       0                 1    Barnard   \n",
       "368        Versie Parker  3.464078       0                 0    Barnard   \n",
       "389       Linda Escobedo  3.110268       0                 1  Wellesley   \n",
       "402         Mary Messing  3.243789       0                 0  Wellesley   \n",
       "436         Sheila Smith  3.633974       0                 2    Barnard   \n",
       "440       Ellen Scrivner  3.322853       0                 2  Wellesley   \n",
       "444         Marisa Kyser  3.353890       0                 2  Wellesley   \n",
       "452    Patricia Cendejas  3.150307       0                 0    Barnard   \n",
       "455             Sue Reed  3.206812       0                 0  Wellesley   \n",
       "495      Nancy Clayborne  3.028616       0                 1  Wellesley   \n",
       "\n",
       "     Accepted  \n",
       "57          0  \n",
       "59          0  \n",
       "74          1  \n",
       "114         0  \n",
       "150         1  \n",
       "169         0  \n",
       "175         0  \n",
       "208         0  \n",
       "241         1  \n",
       "287         1  \n",
       "333         1  \n",
       "361         1  \n",
       "368         0  \n",
       "389         0  \n",
       "402         0  \n",
       "436         0  \n",
       "440         1  \n",
       "444         0  \n",
       "452         0  \n",
       "455         0  \n",
       "495         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Gender'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = StandardDataset(df,\n",
    "                               label_name='Accepted',\n",
    "                               favorable_classes=[1],\n",
    "                               protected_attribute_names=['Gender'],\n",
    "                               privileged_classes=[[1]],\n",
    "                               categorical_features=['School'],\n",
    "                               features_to_drop=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_valid = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'Gender': 1}]\n",
    "unprivileged_groups = [{'Gender': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.020896\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/fairml/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "w_train = dataset_orig_train.instance_weights.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_orig_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy()\n",
    "dataset_orig_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "dataset_orig_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no reweighing) = 0.7240\n",
      "Optimal classification threshold (no reweighing) = 0.3466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                             dataset_orig_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no reweighing) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no reweighing) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from original testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:00<00:00, 217.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used = 0.3466\n",
      "Balanced accuracy = 0.6514\n",
      "Statistical parity difference = -0.0694\n",
      "Disparate impact = 0.8780\n",
      "Average odds difference = 0.0977\n",
      "Equal opportunity difference = 0.2500\n",
      "Theil index = 0.1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:00<00:00, 237.52it/s]/anaconda3/envs/fairml/lib/python3.5/site-packages/aif360/metrics/dataset_metric.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      "100%|██████████| 100/100 [00:00<00:00, 261.65it/s]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Predictions from original testing data\"))\n",
    "bal_acc_arr_orig = []\n",
    "disp_imp_arr_orig = []\n",
    "avg_odds_diff_arr_orig = []\n",
    "\n",
    "print(\"Classification threshold used = %.4f\" % best_class_thresh)\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    \n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    \n",
    "    fav_inds = dataset_orig_test_pred.scores > thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "    \n",
    "    metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                                      unprivileged_groups, privileged_groups,\n",
    "                                      disp = disp)\n",
    "\n",
    "    bal_acc_arr_orig.append(metric_test_bef[\"Balanced accuracy\"])\n",
    "    avg_odds_diff_arr_orig.append(metric_test_bef[\"Average odds difference\"])\n",
    "    disp_imp_arr_orig.append(metric_test_bef[\"Disparate impact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/fairml/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scale_transf = StandardScaler()\n",
    "X_train = scale_transf.fit_transform(dataset_transf_train.features)\n",
    "y_train = dataset_transf_train.labels.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train,\n",
    "        sample_weight=dataset_transf_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transf_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_transf.fit_transform(dataset_transf_test_pred.features)\n",
    "y_test = dataset_transf_test_pred.labels\n",
    "dataset_transf_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from transformed testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:00<00:00, 292.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used = 0.3466\n",
      "Balanced accuracy = 0.6675\n",
      "Statistical parity difference = -0.0486\n",
      "Disparate impact = 0.9114\n",
      "Average odds difference = 0.1148\n",
      "Equal opportunity difference = 0.2500\n",
      "Theil index = 0.1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:00<00:00, 125.22it/s]/anaconda3/envs/fairml/lib/python3.5/site-packages/aif360/metrics/dataset_metric.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      "100%|██████████| 100/100 [00:00<00:00, 182.01it/s]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Predictions from transformed testing data\"))\n",
    "bal_acc_arr_transf = []\n",
    "disp_imp_arr_transf = []\n",
    "avg_odds_diff_arr_transf = []\n",
    "\n",
    "print(\"Classification threshold used = %.4f\" % best_class_thresh)\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    \n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    \n",
    "    fav_inds = dataset_transf_test_pred.scores > thresh\n",
    "    dataset_transf_test_pred.labels[fav_inds] = dataset_transf_test_pred.favorable_label\n",
    "    dataset_transf_test_pred.labels[~fav_inds] = dataset_transf_test_pred.unfavorable_label\n",
    "    \n",
    "    metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                                      unprivileged_groups, privileged_groups,\n",
    "                                      disp = disp)\n",
    "\n",
    "    bal_acc_arr_transf.append(metric_test_aft[\"Balanced accuracy\"])\n",
    "    avg_odds_diff_arr_transf.append(metric_test_aft[\"Average odds difference\"])\n",
    "    disp_imp_arr_transf.append(metric_test_aft[\"Disparate impact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (fairml)",
   "language": "python",
   "name": "fairml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
